{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# When invoking the class, we want to scrape a set number of ip proxies and devices to use for un-detectable scraping\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class webScraper:\n",
    "    '''\n",
    "    Given a csv file of user agents and url or proxy site, this will create a web-scraper that will use\n",
    "    these details to mask your details when scraping given sites\n",
    "    '''\n",
    "    device_list = []\n",
    "    proxy_site = \"\"\n",
    "    address_list = []\n",
    "    \n",
    "    def __init__(self, csv_path, proxy_site):\n",
    "        \"\"\"\n",
    "        Supply csv of User Agents and Proxy website\n",
    "        This will then initialise the class, setting up each for use in masking.\n",
    "        \"\"\"\n",
    "        self.proxy_site = proxy_site\n",
    "        device_df = pd.read_csv(csv_path)\n",
    "        print(\"CSV Read.\")\n",
    "        self.device_list = [str(x) for x in device_df[\"user-agent\"].tolist()]\n",
    "        print(\"Initialisation Complete.\")\n",
    "        \n",
    "    def get_html_response(self, url):\n",
    "        \"\"\"\n",
    "        Give url to scrape, this will return the json response and the html parsed page response generated from\n",
    "        beautiful soup. Uses proxy and user agent details to mask details.\n",
    "        \"\"\"\n",
    "        if len(self.address_list) == 0:\n",
    "            print(\"Updating Proxy List\")\n",
    "            self.update_live_address_list()\n",
    "        \n",
    "        # Get proxy and user agent to use for this request\n",
    "        proxy = self.get_random_proxy()\n",
    "        user_agent = self.get_random_user_agent()\n",
    "            \n",
    "        print(\"Getting HTML Response...\")\n",
    "        print(\"Proxy: {}\".format(proxy))\n",
    "        print(\"User Agent: {}\".format(user_agent))\n",
    "        \n",
    "        response_json = \"\"\n",
    "        page = \"\"\n",
    "        while page == \"\":\n",
    "            # Use proxy and user agent to get page\n",
    "            try:\n",
    "                # If proxy fails connection, remove from list\n",
    "                response = requests.get(url,proxies={\"http\": str(\"http://\" + proxy),\"https\": str(\"http://\" + proxy)})\n",
    "            except:\n",
    "                print(\"Skipping, Connection Failed.\")\n",
    "                proxy = self.replace_proxy(proxy)\n",
    "                print(\"New Proxy selected\")\n",
    "        # Get responses to return\n",
    "        response_json = response.json()\n",
    "        page = soup(response.content, \"html.parser\")\n",
    "        return response_json, page\n",
    "    \n",
    "    def replace_proxy(self,proxy):\n",
    "        \"\"\"\n",
    "        Removes faulty proxy from proxy list, and gives new proxy.\n",
    "        Also gets a new set of proxies if the list has become empty.\n",
    "        \"\"\"\n",
    "        old_list = self.address_list\n",
    "        old_list.remove(proxy)\n",
    "        new_list = old_list\n",
    "        self.address_list = new_list\n",
    "        if len(new_list) == 0:\n",
    "            self.update_live_address_list()\n",
    "        else:\n",
    "            proxy = self.get_random_proxy()\n",
    "        return proxy\n",
    "        \n",
    "    def update_live_address_list(self):\n",
    "        user_agent = self.get_random_user_agent()\n",
    "        response = requests.get(self.proxy_site,headers={'User-Agent':\"Custom\"})\n",
    "        page = soup(response.content, \"html.parser\")\n",
    "        table_data = page.findAll(\"tr\")\n",
    "        ip_addresses = []\n",
    "        for i in range(1,51):\n",
    "            row_data = table_data[i].findAll(\"td\")\n",
    "            proxy_type = str(row_data[4].text)\n",
    "            https = str(row_data[6].text)\n",
    "            if proxy_type == \"elite proxy\" and https == \"yes\":\n",
    "                ip_address = str(row_data[0].text)\n",
    "                port = str(row_data[1].text)\n",
    "                final_address = ip_address + \":\" + port\n",
    "                ip_addresses.append(final_address)\n",
    "        print(\"IP Addresses: {}\".format(ip_addresses))\n",
    "        self.address_list = ip_addresses\n",
    "      \n",
    "    def get_random_proxy(self):\n",
    "        return random.choice(self.address_list)\n",
    "    \n",
    "    def get_random_user_agent(self):\n",
    "        return random.choice(self.device_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
